{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  3 15:57:41 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.72       Driver Version: 512.72       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   59C    P8     5W /  N/A |    685MiB /  4096MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2784    C+G   ...d\\runtime\\WeChatAppEx.exe    N/A      |\n",
      "|    0   N/A  N/A      5872    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     23080    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     28036    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     33168    C+G   ...kzcwy\\mcafee-security.exe    N/A      |\n",
      "|    0   N/A  N/A     35016    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     35444    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     42628    C+G   ...arp.BrowserSubprocess.exe    N/A      |\n",
      "|    0   N/A  N/A     42672    C+G   ...t\\runtime\\WeChatAppEx.exe    N/A      |\n",
      "|    0   N/A  N/A     46056    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     46840    C+G   ...tracted\\WechatBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A     48988    C+G   ...p-3.0.0\\GitHubDesktop.exe    N/A      |\n",
      "|    0   N/A  N/A     56568    C+G   ...ChatStore\\WeChatStore.exe    N/A      |\n",
      "|    0   N/A  N/A     57732    C+G   ...30c\\win32\\CinemaColor.exe    N/A      |\n",
      "|    0   N/A  N/A     58696    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     60368    C+G   ...VMWare\\x64\\mksSandbox.exe    N/A      |\n",
      "|    0   N/A  N/A     61116    C+G   ...s\\RStudio\\bin\\rstudio.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.20.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\桌面\\学习资料\\贷款违约预测\n",
      "d:\\桌面\\学习资料\\贷款违约预测\n",
      "d:\\桌面\\学习资料\n",
      "d:\\桌面\\学习资料\\贷款违约预测\n"
     ]
    }
   ],
   "source": [
    "import  os\n",
    "\n",
    "print(os.getcwd())#获取当前工作目录路径\n",
    "print(os.path.abspath('.')) #获取当前工作目录路径\n",
    "# print os.path.abspath('test.txt') #获取当前目录文件下的工作目录路径\n",
    "print(os.path.abspath('..')) #获取当前工作的父目录 ！注意是父目录路径\n",
    "print(os.path.abspath(os.curdir))#获取当前工作目录路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import entropy\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import datetime\n",
    "import time\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nowtime = datetime.date.today()\n",
    "nowtime = str(nowtime)[-5:]\n",
    "print(nowtime)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##读取数据\n",
    "def load_dataset(DATA_PATH):\n",
    "    train_label = pd.read_csv(DATA_PATH + 'train.csv')['isDefault']\n",
    "    train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "    test = pd.read_csv(DATA_PATH + 'testA.csv')\n",
    "    \n",
    "    feats = [f for f in train.columns if f not in ['isDefault']]\n",
    "    # train = train[feats]\n",
    "    test = test[feats]\n",
    "    print('train.shape', train.shape)\n",
    "    print('test.shape', test.shape)\n",
    "\n",
    "    return train_label, train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4505"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 处理时间\n",
    "def transform_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400 * day + 3600 * hour + 60 * minute + second\n",
    "\n",
    "\n",
    "def transform_day(date1):\n",
    "    date2 = \"2020-01-01\"\n",
    "    date1 = time.strptime(date1, \"%Y-%m-%d\")\n",
    "    date2 = time.strptime(date2, \"%Y-%m-%d\")\n",
    "\n",
    "    # 根据上面需要计算日期还是日期时间，来确定需要几个数组段。下标0表示年，小标1表示月，依次类推...\n",
    "    # date1=datetime.datetime(date1[0],date1[1],date1[2],date1[3],date1[4],date1[5])\n",
    "    # date2=datetime.datetime(date2[0],date2[1],date2[2],date2[3],date2[4],date2[5])\n",
    "    date1 = datetime.datetime(date1[0], date1[1], date1[2])\n",
    "    date2 = datetime.datetime(date2[0], date2[1], date2[2])\n",
    "    # 返回两个变量相差的值，就是相差天数\n",
    "    # print((date2 - date1).days)  # 将天数转成int型\n",
    "    return (date2 - date1).days\n",
    "\n",
    "\n",
    "transform_day('2007-09-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编码方式\n",
    "def labelEncoder_df(df, features):\n",
    "    for i in features:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        df[i] = encoder.fit_transform(df[i])\n",
    "\n",
    "\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "\n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg([('mean', 'mean'), ('beta', 'size'),])\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target,\n",
    "                        self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None,\n",
    "                        self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#employmentLength, earliesCreditLine_month, gradeTrans, subGradeTrans字段处理\n",
    "\n",
    "def employmentLength_deal(x):\n",
    "    #缺失值以-999填充\n",
    "    if x == r'\\N':\n",
    "        result = -999\n",
    "    elif x == -999:\n",
    "        result = -999\n",
    "    elif x == '-999':\n",
    "        result = -999\n",
    "    elif x == '< 1 year':\n",
    "        result = 0.5\n",
    "    elif x == '10+ years':\n",
    "        result = 12\n",
    "    else:\n",
    "        result = int(x.split(' ')[0][0])\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def earliesCreditLine_month_deal(x):\n",
    "    x = x.split('-')[0]\n",
    "    # print(x)\n",
    "    dict = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10,\n",
    "            'Nov': 11, 'Dec': 12}\n",
    "    result = dict[x]\n",
    "    return result\n",
    "\n",
    "\n",
    "def gradeTrans(x):\n",
    "    dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    result = dict[x]\n",
    "    return result\n",
    "\n",
    "\n",
    "def subGradeTrans(x):\n",
    "    dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    result = dict[x[0]]\n",
    "    result = result * 5 + int(x[1])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#暴力提取特征的统计量作为新特征\n",
    "\n",
    "def myEntro(x):\n",
    "    \"\"\"\n",
    "        calculate shanno ent of x\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    x_value_list = set([x[i] for i in range(x.shape[0])])\n",
    "    ent = 0.0\n",
    "    for x_value in x_value_list:\n",
    "        p = float(x[x == x_value].shape[0]) / x.shape[0]\n",
    "        logp = np.log2(p)\n",
    "        ent -= p * logp\n",
    "    #     print(x_value,p,logp)\n",
    "    # print(ent)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def myRms(records):\n",
    "    records = list(records)\n",
    "    \"\"\"\n",
    "    均方根值 反映的是有效值而不是平均值\n",
    "    \"\"\"\n",
    "    return np.math.sqrt(sum([x ** 2 for x in records]) / len(records))\n",
    "\n",
    "\n",
    "def myMode(x):\n",
    "    return np.mean(pd.Series.mode(x))\n",
    "\n",
    "\n",
    "def myQ25(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "\n",
    "def myQ75(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "\n",
    "def myRange(x):\n",
    "    return pd.Series.max(x) - pd.Series.min(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 数据预处理（训练数据与预测数据合并处理）\n",
    "def data_preprocess(DATA_PATH):\n",
    "    train_label, train, test = load_dataset(DATA_PATH=DATA_PATH)\n",
    "    # 拼接数据\n",
    "\n",
    "    data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "    print('初始拼接后：', data.shape)\n",
    "    # n_feat = [f for f in data.columns if f[0] == 'n']\n",
    "\n",
    "    n_feat = ['n0', 'n1', 'n2', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', ]\n",
    "    # nameList = ['min', 'max', 'sum', 'mean', 'median', 'skew', 'std', 'mode', 'range']\n",
    "    # statList = ['min', 'max', 'sum', 'mean', 'median', 'skew', 'std', myMode, myRange]\n",
    "    nameList = ['max', 'sum', 'mean', 'median', 'skew', 'std']\n",
    "    statList = ['max', 'sum', 'mean', 'median', 'skew', 'std']\n",
    "\n",
    "\n",
    "    for i in range(len(nameList)):\n",
    "        data['n_feat_{}'.format(nameList[i])] = data[n_feat].agg(statList[i], axis=1)\n",
    "    print('n特征处理后：', data.shape)\n",
    "\n",
    "\n",
    "    # count编码\n",
    "    count_list = ['subGrade', 'grade', 'postCode', 'regionCode','homeOwnership','title','employmentTitle','employmentLength']\n",
    "    data = count_coding(data, count_list)\n",
    "    print('count编码后：', data.shape)\n",
    "    ### 用数值特征对类别特征做统计刻画，随便挑了几个跟price相关性最高的匿名特征\n",
    "    cross_cat = ['subGrade', 'grade', 'employmentLength', 'term', 'homeOwnership', 'postCode', 'regionCode','employmentTitle','title']\n",
    "    cross_num = ['dti', 'revolBal','revolUtil', 'ficoRangeHigh', 'interestRate', 'loanAmnt', 'installment', 'annualIncome', 'n14',\n",
    "                 'n2', 'n6', 'n9', 'n5', 'n8']\n",
    "\n",
    "    data[['employmentLength']].fillna(-999, inplace=True)\n",
    "\n",
    "#     data = cross_cat_num(data, cross_num, cross_cat)  # 一阶交叉\n",
    "#     print('一阶特征处理后：', data.shape)\n",
    "#     data = cross_qua_cat_num(data)  # 二阶交叉\n",
    "#     print('二阶特征处理后：', data.shape)\n",
    "    # 缺失值处理\n",
    "    for temp in count_list:\n",
    "        del data[temp+'_count']\n",
    "    # num_fill_col = ['employmentLength', 'postCode', ]\n",
    "    cols = ['employmentTitle', 'employmentLength', 'postCode', 'dti', 'pubRecBankruptcies', 'revolUtil', 'title',\n",
    "            'n0', 'n1', 'n2', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14']\n",
    "    for col in cols:\n",
    "        data[col].fillna(r'\\N', inplace=True)\n",
    "    cols = [f for f in cols if f not in ['employmentLength']]\n",
    "    for col in cols:\n",
    "        data[col].replace({r'\\N': -999}, inplace=True)\n",
    "        data[col] = data[col]\n",
    "    # print('缺失值情况：', data.isnull().sum())\n",
    "\n",
    "    data['grade'] = data['grade'].apply(lambda x: gradeTrans(x))\n",
    "    data['subGrade'] = data['subGrade'].apply(lambda x: subGradeTrans(x))\n",
    "    print('1data.shape', data.shape)\n",
    "\n",
    "    data['employmentLength'] = data['employmentLength'].apply(lambda x: employmentLength_deal(x))\n",
    "    data['issueDate_year'] = data['issueDate'].apply(lambda x: int(x.split('-')[0]))\n",
    "    data['issueDate_month'] = data['issueDate'].apply(lambda x: int(x.split('-')[1]))\n",
    "    data['issueDate_day'] = data['issueDate'].apply(lambda x: transform_day(x))\n",
    "    data['issueDate_week'] = data['issueDate_day'].apply(lambda x: int(x % 7) + 1)\n",
    "\n",
    "    print('2_data.shape', data.shape)\n",
    "    data['earliesCreditLine_year'] = data['earliesCreditLine'].apply(lambda x: 2020 - (int(x.split('-')[-1])))\n",
    "    data['earliesCreditLine_month'] = data['earliesCreditLine'].apply(lambda x: earliesCreditLine_month_deal(x))\n",
    "    data['earliesCreditLine_Allmonth'] = data['earliesCreditLine_year'] * 12 - data['earliesCreditLine_month']\n",
    "    del data['issueDate'], data['earliesCreditLine']\n",
    "\n",
    "    print('预处理完毕', data.shape)\n",
    "\n",
    "    return data, train_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常规的 target encoder 目标编码代码容易造成过拟合，因此引入 5 折交叉验证的形式改进编码\n",
    "# 即将样本分为 5 块（fold），每一 fold 中的该高基数无序特征类别由其它 4 个 fold 中的类别对应标签平均值替换表示：\n",
    "\n",
    "def kfold_stats_feature(train, test, feats, k):\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=6666)  # 这里最好和后面模型的K折交叉验证保持一致\n",
    "\n",
    "    train['fold'] = None\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['isDefault'])):\n",
    "        train.loc[val_idx, 'fold'] = fold_\n",
    "\n",
    "    kfold_features = []\n",
    "    for feat in feats:\n",
    "        nums_columns = ['isDefault']\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            kfold_features.append(colname)\n",
    "            train[colname] = None\n",
    "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['isDefault'])):\n",
    "                tmp_trn = train.iloc[trn_idx]\n",
    "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
    "                tmp = train.loc[train.fold == fold_, [feat]]\n",
    "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n",
    "                # fillna\n",
    "                global_mean = train[f].mean()\n",
    "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
    "            train[colname] = train[colname].astype(float)\n",
    "\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            test[colname] = None\n",
    "            order_label = train.groupby([feat])[f].mean()\n",
    "            test[colname] = test[feat].map(order_label)\n",
    "            # fillna\n",
    "            global_mean = train[f].mean()\n",
    "            test[colname] = test[colname].fillna(global_mean)\n",
    "            test[colname] = test[colname].astype(float)\n",
    "    del train['fold']\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GridSearch(clf, params, X, y):\n",
    "    cscv = GridSearchCV(clf, params, scoring='roc_auc', n_jobs=4, cv=10)\n",
    "    cscv.fit(X, y)\n",
    "    print(cscv.cv_results_)\n",
    "    print(cscv.best_params_)\n",
    "    print(cscv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义离散型特征和连续型特征交叉特征统计函数\n",
    "def cross_cat_num(df, num_col, cat_col):\n",
    "    for f1 in tqdm(cat_col):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_col):\n",
    "            feat = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max', '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "            })\n",
    "            df = df.merge(feat, on=f1, how='left')\n",
    "    return (df)\n",
    "\n",
    "#类别特征的二级交叉\n",
    "def cross_qua_cat_num(df):\n",
    "    for f_pair in tqdm([\n",
    "        ['subGrade', 'regionCode'], ['grade', 'regionCode'], ['subGrade', 'postCode'], ['grade', 'postCode'], ['employmentTitle','title'],\n",
    "        ['regionCode','title'], ['postCode','title'], ['homeOwnership','title'], ['homeOwnership','employmentTitle'],['homeOwnership','employmentLength'],\n",
    "        ['regionCode', 'postCode']\n",
    "    ]):\n",
    "        ### 共现次数\n",
    "        df['_'.join(f_pair) + '_count'] = df.groupby(f_pair)['id'].transform('count')\n",
    "        ### n unique、熵\n",
    "        df = df.merge(df.groupby(f_pair[0], as_index=False)[f_pair[1]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[0], f_pair[1]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[0], f_pair[1]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[0], how='left')\n",
    "        df = df.merge(df.groupby(f_pair[1], as_index=False)[f_pair[0]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[1], f_pair[0]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[1], f_pair[0]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[1], how='left')\n",
    "        ### 比例偏好\n",
    "        df['{}_in_{}_prop'.format(f_pair[0], f_pair[1])] = df['_'.join(f_pair) + '_count'] / df[f_pair[1] + '_count']\n",
    "        df['{}_in_{}_prop'.format(f_pair[1], f_pair[0])] = df['_'.join(f_pair) + '_count'] / df[f_pair[0] + '_count']\n",
    "    return (df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### count编码\n",
    "def count_coding(df, fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#新构造的有实际意义的特征\n",
    "def gen_basicFea(data):\n",
    "    data['avg_income'] = data['annualIncome'] / data['employmentLength']\n",
    "    data['total_income'] = data['annualIncome'] * data['employmentLength']\n",
    "    data['avg_loanAmnt'] = data['loanAmnt'] / data['term']\n",
    "    data['mean_interestRate'] = data['interestRate'] / data['term']\n",
    "    data['all_installment'] = data['installment'] * data['term']\n",
    "\n",
    "    data['rest_money_rate'] = data['avg_loanAmnt'] / (data['annualIncome'] + 0.1)  # 287个收入为0\n",
    "    data['rest_money'] = data['annualIncome'] - data['avg_loanAmnt']\n",
    "\n",
    "    data['closeAcc'] = data['totalAcc'] - data['openAcc']\n",
    "    data['ficoRange_mean'] = (data['ficoRangeHigh'] + data['ficoRangeLow']) / 2\n",
    "    del data['ficoRangeHigh'], data['ficoRangeLow']\n",
    "\n",
    "    data['rest_pubRec'] = data['pubRec'] - data['pubRecBankruptcies']\n",
    "\n",
    "    data['rest_Revol'] = data['loanAmnt'] - data['revolBal']\n",
    "\n",
    "    data['dis_time'] = data['issueDate_year'] - (2020 - data['earliesCreditLine_year'])\n",
    "    for col in ['employmentTitle', 'grade', 'subGrade', 'regionCode', 'issueDate_month', 'postCode']:\n",
    "        data['{}_count'.format(col)] = data.groupby([col])['id'].transform('count')\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plotroc(train_y, train_pred, test_y, val_pred):\n",
    "    lw = 2\n",
    "    ##train\n",
    "    fpr, tpr, thresholds = roc_curve(train_y.values, train_pred, pos_label=1.0)\n",
    "    train_auc_value = roc_auc_score(train_y.values, train_pred)\n",
    "    ##valid\n",
    "    fpr, tpr, thresholds = roc_curve(test_y.values, val_pred, pos_label=1.0)\n",
    "    valid_auc_value = roc_auc_score(test_y.values, val_pred)\n",
    "\n",
    "    return train_auc_value, valid_auc_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "def xgb_model(train, target, test, k):\n",
    "    # saveFeature_df = pd.read_csv('../feature/xgb_920_556_5_score.csv',header=None)\n",
    "    #saveFeature_df2 = pd.read_csv('../feature/xgb_09-20_74_0.8_0.6_5_score.csv',header=None)\n",
    "    \n",
    "    # saveFeature_df.columns=['feature','score']\n",
    "    #saveFeature_df2.columns=['feature','score']\n",
    "\n",
    "    #saveFeature_list = list(saveFeature_df['feature'].values)\n",
    "    #saveFeature_list2 = list(saveFeature_df2['feature'].values)\n",
    "    \n",
    "    # saveFeature_list = list(saveFeature_df[saveFeature_df['score']>10]['feature'])\n",
    "    saveFeature_list=list(train.columns)\n",
    "    feats = [f for f in saveFeature_list if f not in ['id', 'isDefault']]\n",
    "    feaNum = len(feats)\n",
    "    print('Current num of features:', len(feats))\n",
    "\n",
    "    seeds = [2020,666666,188888]\n",
    "    output_preds = 0\n",
    "    xgb_oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "    for seed in seeds:\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "        offline_score = []\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        params = {'booster': 'gbtree',\n",
    "                  'objective': 'binary:logistic',\n",
    "                  'eval_metric': 'auc',\n",
    "                  'min_child_weight': 5,\n",
    "                  'max_depth': 8,\n",
    "                  'subsample': ss,\n",
    "                  'colsample_bytree': fs,\n",
    "                  'eta': 0.01,\n",
    "\n",
    "                  'seed': seed,\n",
    "                  'nthread': -1,\n",
    "\n",
    "                  'tree_method': 'gpu_hist'\n",
    "                  }\n",
    "        for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "            \n",
    "            train_y, test_y = target[train_index], target[test_index]\n",
    "            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "            train_matrix = xgb.DMatrix(train_X, label=train_y, missing=np.nan)\n",
    "            valid_matrix = xgb.DMatrix(test_X, label=test_y, missing=np.nan)\n",
    "            test_matrix = xgb.DMatrix(test[feats], missing=np.nan)\n",
    "            watchlist = [(train_matrix, 'train'), (valid_matrix, 'eval')]\n",
    "            model = xgb.train(params, train_matrix, num_boost_round=100000, evals=watchlist, verbose_eval=100,\n",
    "                              early_stopping_rounds=600)\n",
    "            val_pred = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            train_pred = model.predict(train_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            xgb_oof_probs[test_index] += val_pred / len(seeds)\n",
    "            # oof_probs[test_index] += val_pred\n",
    "            test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "            # 绘制roc曲线\n",
    "            train_auc_value, valid_auc_value = plotroc(train_y, train_pred, test_y, val_pred)\n",
    "            print('train_auc:{},valid_auc{}'.format(train_auc_value, valid_auc_value))\n",
    "            offline_score.append(valid_auc_value)\n",
    "            print(offline_score)\n",
    "            output_preds += test_pred / k / len(seeds)\n",
    "            \n",
    "#             sub_df = test[['id']].copy()\n",
    "#             sub_df['isDefault'] = output_preds\n",
    "#             off = test[['id']].copy()\n",
    "#             subVal_df = train[['id']].copy()\n",
    "#             subVal_df.loc[test_index,'isDefault'] = xgb_oof_probs[test_index]\n",
    "#             outpath = '../user_data/fold/'\n",
    "#             fold_score = round(valid_auc_value, 5)\n",
    "#             sub_df.to_csv( outpath + str(fold_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_xgb.csv'.format(i, kflod_num),\n",
    "#             index=False)\n",
    "#             subVal_df.to_csv(outpath + str(fold_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_xgbVal.csv'.format(i, kflod_num),\n",
    "#             index=False)\n",
    "\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"Feature\"] = model.get_fscore().keys()\n",
    "            fold_importance_df[\"importance\"] = model.get_fscore().values()\n",
    "            fold_importance_df[\"fold\"] = i + 1\n",
    "\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "#             print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "#             feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "#             feature_sorted.to_csv('../feature/xgb_{}_{}_{}_{}_score.csv'.format(i+1,nowtime, feaNum, kflod_num))\n",
    "\n",
    "#             if i==3:\n",
    "#                 break\n",
    "#             gc.collect()\n",
    "\n",
    "\n",
    "        print('all_auc:', roc_auc_score(target.values, oof_probs))\n",
    "        print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "        feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "        feature_sorted.to_csv('xgb_importance.csv')\n",
    "        top_features = feature_sorted.index\n",
    "        print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "    return output_preds, xgb_oof_probs, np.mean(offline_score), feaNum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据...\n",
      "train.shape (800000, 47)\n",
      "test.shape (200000, 46)\n",
      "初始拼接后： (1000000, 47)\n",
      "n特征处理后： (1000000, 53)\n",
      "count编码后： (1000000, 61)\n",
      "1data.shape (1000000, 53)\n",
      "2_data.shape (1000000, 57)\n",
      "预处理完毕 (1000000, 58)\n",
      "开始特征工程...\n",
      "data.shape (1000000, 74)\n",
      "开始模型训练...\n",
      "num0:mean_encode train.shape (800000, 84) (200000, 84)\n",
      "num1:target_encode train.shape (800000, 89) (200000, 89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:10<00:00, 62.12s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num2:target_encode train.shape (800000, 109) (200000, 109)\n",
      "输入数据维度： (800000, 104) (200000, 104)\n",
      "Current num of features: 102\n",
      "[0]\ttrain-auc:0.71510\teval-auc:0.70796\n",
      "[100]\ttrain-auc:0.73560\teval-auc:0.72415\n",
      "[200]\ttrain-auc:0.74143\teval-auc:0.72737\n",
      "[300]\ttrain-auc:0.74712\teval-auc:0.73013\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "No debugger available, can not send 'disconnect'. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    DATA_PATH = 'd:/桌面/学习资料/贷款违约预测/'\n",
    "    print('读取数据...')\n",
    "    data, train_label = data_preprocess(DATA_PATH=DATA_PATH)\n",
    "\n",
    "    print('开始特征工程...')\n",
    "    data = gen_basicFea(data)\n",
    "    \n",
    "    print('data.shape', data.shape)\n",
    "    print('开始模型训练...')\n",
    "    train = data[~data['isDefault'].isnull()].copy()\n",
    "    target = train_label\n",
    "    test = data[data['isDefault'].isnull()].copy()\n",
    "\n",
    "    target_encode_cols = ['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']\n",
    "\n",
    "    kflod_num = 5\n",
    "    ss = 0.8\n",
    "    fs = 0.4\n",
    "\n",
    "    class_list = ['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']\n",
    "    MeanEnocodeFeature = class_list  # 声明需要平均数编码的特征\n",
    "    ME = MeanEncoder(MeanEnocodeFeature, target_type='classification')  # 声明平均数编码的类\n",
    "    train = ME.fit_transform(train, target)  # 对训练数据集的X和y进行拟合\n",
    "    # x_train_fav = ME.fit_transform(x_train,y_train_fav)#对训练数据集的X和y进行拟合\n",
    "    test = ME.transform(test)  # 对测试集进行编码\n",
    "    print('num0:mean_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "    train, test = kfold_stats_feature(train, test, target_encode_cols, kflod_num)\n",
    "    print('num1:target_encode train.shape', train.shape, test.shape)\n",
    "    ### target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "    enc_cols = []\n",
    "    stats_default_dict = {\n",
    "        'max': train['isDefault'].max(),\n",
    "        'min': train['isDefault'].min(),\n",
    "        'median': train['isDefault'].median(),\n",
    "        'mean': train['isDefault'].mean(),\n",
    "        'sum': train['isDefault'].sum(),\n",
    "        'std': train['isDefault'].std(),\n",
    "        'skew': train['isDefault'].skew(),\n",
    "        'kurt': train['isDefault'].kurt(),\n",
    "        'mad': train['isDefault'].mad()\n",
    "    }\n",
    "    ### 暂且选择这三种编码\n",
    "    enc_stats = ['max', 'min', 'skew', 'std']\n",
    "    skf = KFold(n_splits=kflod_num, shuffle=True, random_state=6666)\n",
    "    for f in tqdm(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']):\n",
    "        enc_dict = {}\n",
    "        for stat in enc_stats:\n",
    "            enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "            train['{}_target_{}'.format(f, stat)] = 0\n",
    "            test['{}_target_{}'.format(f, stat)] = 0\n",
    "            enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "        for i, (trn_idx, val_idx) in enumerate(skf.split(train, target)):\n",
    "            trn_x, val_x = train.iloc[trn_idx].reset_index(drop=True), train.iloc[val_idx].reset_index(drop=True)\n",
    "            enc_df = trn_x.groupby(f, as_index=False)['isDefault'].agg(enc_dict)\n",
    "            val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "            test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "            for stat in enc_stats:\n",
    "                val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                    stats_default_dict[stat])\n",
    "                test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                    stats_default_dict[stat])\n",
    "                train.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values\n",
    "                test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits\n",
    "\n",
    "    print('num2:target_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "    train.drop(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title'], axis=1, inplace=True)\n",
    "    test.drop(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title'], axis=1, inplace=True)\n",
    "    print('输入数据维度：', train.shape, test.shape)\n",
    "    \n",
    "    xgb_preds, xgb_oof, xgb_score, feaNum = xgb_model(train=train, target=target, test=test, k=kflod_num)\n",
    "\n",
    "    lgb_score = round(xgb_score, 5)\n",
    "    sub_df = test[['id']].copy()\n",
    "    sub_df['isDefault'] = xgb_preds\n",
    "    off = test[['id']].copy()\n",
    "    subVal_df = train[['id']].copy()\n",
    "    subVal_df['isDefault'] = xgb_oof\n",
    "    outpath = '../user_data/'\n",
    "\n",
    "    all_auc_score = roc_auc_score(train_label, subVal_df['isDefault'])\n",
    "    print('整体指标得分：', all_auc_score)\n",
    "    all_auc_score = round(all_auc_score, 5)\n",
    "\n",
    "    sub_df.to_csv('xgb1.csv',index=False)\n",
    "    subVal_df.to_csv('xgb1Val.csv',index=False)\n",
    "    # sub_df.to_csv(\n",
    "    #     outpath + str(all_auc_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_{}_xgb.csv'.format(ss, fs,\n",
    "    #                                                                                                   kflod_num),\n",
    "    #     index=False)\n",
    "    # subVal_df.to_csv(\n",
    "    #     outpath + str(all_auc_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_{}_subVal.csv'.format(ss, fs,\n",
    "    #                                                                                                      kflod_num),\n",
    "    #     index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model2(train, target, test, k):\n",
    "    saveFeature_df = pd.read_csv('xgb_importance.csv',header=None)\n",
    "#     saveFeature_df2 = pd.read_csv('xgb_09-20_74_0.8_0.6_5_score.csv',header=None)\n",
    "    \n",
    "    saveFeature_df.columns=['feature','score']\n",
    "#     saveFeature_df2.columns=['feature','score']\n",
    "\n",
    "    saveFeature_list = list(saveFeature_df['feature'].values)\n",
    "#     saveFeature_list2 = list(saveFeature_df2['feature'].values)\n",
    "    \n",
    "    saveFeature_list = list(saveFeature_df[saveFeature_df['score']>0]['feature'])\n",
    "    saveFeature_list=list(train.columns)\n",
    "    feats = [f for f in saveFeature_list if f not in ['id', 'isDefault']]\n",
    "    feaNum = len(feats)\n",
    "    print('Current num of features:', len(feats))\n",
    "\n",
    "    seeds = [2020,666666,188888]\n",
    "    output_preds = 0\n",
    "    xgb_oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "    for seed in seeds:\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "        offline_score = []\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        params = {'booster': 'gbtree',\n",
    "                  'objective': 'binary:logistic',\n",
    "                  'eval_metric': 'auc',\n",
    "                  'min_child_weight': 5,\n",
    "                  'max_depth': 8,\n",
    "                  'subsample': ss,\n",
    "                  'colsample_bytree': fs,\n",
    "                  'eta': 0.01,\n",
    "\n",
    "                  'seed': seed,\n",
    "                  'nthread': -1,\n",
    "\n",
    "                  'tree_method': 'gpu_hist'\n",
    "                  }\n",
    "        for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "            \n",
    "            train_y, test_y = target[train_index], target[test_index]\n",
    "            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "            train_matrix = xgb.DMatrix(train_X, label=train_y, missing=np.nan)\n",
    "            valid_matrix = xgb.DMatrix(test_X, label=test_y, missing=np.nan)\n",
    "            test_matrix = xgb.DMatrix(test[feats], missing=np.nan)\n",
    "            watchlist = [(train_matrix, 'train'), (valid_matrix, 'eval')]\n",
    "            model = xgb.train(params, train_matrix, num_boost_round=100000, evals=watchlist, verbose_eval=100,\n",
    "                              early_stopping_rounds=600)\n",
    "            val_pred = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            train_pred = model.predict(train_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            xgb_oof_probs[test_index] += val_pred / len(seeds)\n",
    "            # oof_probs[test_index] += val_pred\n",
    "            test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "            # 绘制roc曲线\n",
    "            train_auc_value, valid_auc_value = plotroc(train_y, train_pred, test_y, val_pred)\n",
    "            print('train_auc:{},valid_auc{}'.format(train_auc_value, valid_auc_value))\n",
    "            offline_score.append(valid_auc_value)\n",
    "            print(offline_score)\n",
    "            output_preds += test_pred / k / len(seeds)\n",
    "            \n",
    "#             sub_df = test[['id']].copy()\n",
    "#             sub_df['isDefault'] = output_preds\n",
    "#             off = test[['id']].copy()\n",
    "#             subVal_df = train[['id']].copy()\n",
    "#             subVal_df.loc[test_index,'isDefault'] = xgb_oof_probs[test_index]\n",
    "#             outpath = '../user_data/fold/'\n",
    "#             fold_score = round(valid_auc_value, 5)\n",
    "#             sub_df.to_csv( outpath + str(fold_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_xgb.csv'.format(i, kflod_num),\n",
    "#             index=False)\n",
    "#             subVal_df.to_csv(outpath + str(fold_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_xgbVal.csv'.format(i, kflod_num),\n",
    "#             index=False)\n",
    "\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"Feature\"] = model.get_fscore().keys()\n",
    "            fold_importance_df[\"importance\"] = model.get_fscore().values()\n",
    "            fold_importance_df[\"fold\"] = i + 1\n",
    "\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "#             print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "#             feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "#             feature_sorted.to_csv('../feature/xgb_{}_{}_{}_{}_score.csv'.format(i+1,nowtime, feaNum, kflod_num))\n",
    "\n",
    "#             if i==3:\n",
    "#                 break\n",
    "#             gc.collect()\n",
    "\n",
    "\n",
    "        print('all_auc:', roc_auc_score(target.values, oof_probs))\n",
    "        print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "        feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "#         feature_sorted.to_csv('../feature/xgb_importance.csv')\n",
    "        top_features = feature_sorted.index\n",
    "        print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "    return output_preds, xgb_oof_probs, np.mean(offline_score), feaNum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape (1000000, 74)\n",
      "开始模型训练...\n",
      "num0:mean_encode train.shape (800000, 84) (200000, 84)\n",
      "num1:target_encode train.shape (800000, 89) (200000, 89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:18<00:12,  6.04s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "No debugger available, can not send 'disconnect'. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "del train,test\n",
    "gc.collect()\n",
    "print('data.shape', data.shape)\n",
    "print('开始模型训练...')\n",
    "train = data[~data['isDefault'].isnull()].copy()\n",
    "target = train_label\n",
    "test = data[data['isDefault'].isnull()].copy()\n",
    "\n",
    "target_encode_cols = ['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']\n",
    "\n",
    "kflod_num = 5\n",
    "ss = 0.8\n",
    "fs = 0.4\n",
    "\n",
    "class_list = ['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']\n",
    "MeanEnocodeFeature = class_list  # 声明需要平均数编码的特征\n",
    "ME = MeanEncoder(MeanEnocodeFeature, target_type='classification')  # 声明平均数编码的类\n",
    "train = ME.fit_transform(train, target)  # 对训练数据集的X和y进行拟合\n",
    "# x_train_fav = ME.fit_transform(x_train,y_train_fav)#对训练数据集的X和y进行拟合\n",
    "test = ME.transform(test)  # 对测试集进行编码\n",
    "print('num0:mean_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "train, test = kfold_stats_feature(train, test, target_encode_cols, kflod_num)\n",
    "print('num1:target_encode train.shape', train.shape, test.shape)\n",
    "### target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "enc_cols = []\n",
    "stats_default_dict = {\n",
    "    'max': train['isDefault'].max(),\n",
    "    'min': train['isDefault'].min(),\n",
    "    'median': train['isDefault'].median(),\n",
    "    'mean': train['isDefault'].mean(),\n",
    "    'sum': train['isDefault'].sum(),\n",
    "    'std': train['isDefault'].std(),\n",
    "    'skew': train['isDefault'].skew(),\n",
    "    'kurt': train['isDefault'].kurt(),\n",
    "    'mad': train['isDefault'].mad()\n",
    "}\n",
    "### 暂且选择这三种编码\n",
    "enc_stats = ['max', 'min', 'skew', 'std']\n",
    "skf = KFold(n_splits=kflod_num, shuffle=True, random_state=6666)\n",
    "for f in tqdm(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']):\n",
    "    enc_dict = {}\n",
    "    for stat in enc_stats:\n",
    "        enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "        train['{}_target_{}'.format(f, stat)] = 0\n",
    "        test['{}_target_{}'.format(f, stat)] = 0\n",
    "        enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(train, target)):\n",
    "        trn_x, val_x = train.iloc[trn_idx].reset_index(drop=True), train.iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['isDefault'].agg(enc_dict)\n",
    "        val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "        test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "        for stat in enc_stats:\n",
    "            val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                stats_default_dict[stat])\n",
    "            test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                stats_default_dict[stat])\n",
    "            train.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values\n",
    "            test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits\n",
    "\n",
    "print('num2:target_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "train.drop(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title'], axis=1, inplace=True)\n",
    "test.drop(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title'], axis=1, inplace=True)\n",
    "print('输入数据维度：', train.shape, test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_model2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_43196/2511794717.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxgb_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_oof\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaNum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_model2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkflod_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlgb_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msub_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msub_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isDefault'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_preds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_model2' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_preds, xgb_oof, xgb_score, feaNum = xgb_model2(train=train, target=target, test=test, k=kflod_num)\n",
    "\n",
    "lgb_score = round(xgb_score, 5)\n",
    "sub_df = test[['id']].copy()\n",
    "sub_df['isDefault'] = xgb_preds\n",
    "off = test[['id']].copy()\n",
    "subVal_df = train[['id']].copy()\n",
    "subVal_df['isDefault'] = xgb_oof\n",
    "outpath = '../user_data/'\n",
    "\n",
    "all_auc_score = roc_auc_score(train_label, subVal_df['isDefault'])\n",
    "print('整体指标得分：', all_auc_score)\n",
    "all_auc_score = round(all_auc_score, 5)\n",
    "\n",
    "sub_df.to_csv(outpath+'xgb1.csv',index=False)\n",
    "subVal_df.to_csv(outpath+'xgb1Val.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "610a6f344c2137faf927ea819c63f6cee33a2c04455044b28099f39fe9722347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
